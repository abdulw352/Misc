{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will create, train, evaluate, and make predictions on a model using [Apache Beam](https://beam.apache.org/) and [TensorFlow](https://www.tensorflow.org/). In particular, you will train a model to predict the molecular energy based on the number of carbon, hydrogen, oxygen, and nitrogen atoms.\n",
    "\n",
    "This lab is marked as optional because you will not be interacting with Beam-based systems directly in future exercises. Other courses of this specialization also use tools that abstract this layer. Nonetheless, it would be good to be familiar with it since it is used under the hood by TFX which is the main ML pipelines framework that you will use in other labs. Seeing how these systems work will let you explore other codebases that use this tool more freely and even make contributions or bug fixes as you see fit. If you don't know the basics of Beam yet, we encourage you to look at the [Minimal Word Count example here](https://beam.apache.org/get-started/wordcount-example/) for a quick start and use the [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide) to look up concepts if needed.\n",
    "\n",
    "The entire pipeline can be divided into four phases:\n",
    " 1. Data extraction\n",
    " 2. Preprocessing the data\n",
    " 3. Training the model\n",
    " 4. Doing predictions\n",
    "\n",
    "You will focus particularly on Phase 2 (Preprocessing) and a bit of Phase 4 (Predictions) because these use Beam in its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Some of the packages used in this lab are not compatible with the\n",
    "# default Python version in Colab (Python3.8 as of January 2023).\n",
    "# The commands below will setup the environment to use Python3.7 instead.\n",
    "# Please *DO NOT* restart the runtime after running these.\n",
    "\n",
    "# Install packages needed to downgrade to Python3.7\n",
    "!apt-get install python3.7 python3.7-distutils python3-pip\n",
    "\n",
    "# Configure the Colab environment to use Python3.7 by default\n",
    "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the scripts\n",
    "!wget https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz\n",
    "\n",
    "# Unzip the archive\n",
    "!tar -xvzf molecules.tar.gz --one-top-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the scripts\n",
    "!wget https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz\n",
    "\n",
    "# Unzip the archive\n",
    "!tar -xvzf molecules.tar.gz --one-top-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory\n",
    "WORK_DIR = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data-extractor.py` file extracts and decompresses the specified SDF files. In later steps, the example preprocesses these files and uses the data to train and evaluate the machine learning model. The file extracts the SDF files from the public source and stores them in a subdirectory inside the specified working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the help documentation. You can ignore references to GCP because you will be running everything in Colab.\n",
    "!python ./molecules/data-extractor.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data extractor\n",
    "!python ./molecules/data-extractor.py --max-data-files 1 --work-dir={WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List working directory\n",
    "!ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one record\n",
    "!sed '/$$$$/q' {WORK_DIR}/data/00000001_00025000.sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Preprocessing\n",
    "\n",
    "The next script: `preprocess.py` uses an Apache Beam pipeline to preprocess the data. The pipeline performs the following preprocessing actions:\n",
    "\n",
    "1. Reads and parses the extracted SDF files.\n",
    "2. Counts the number of different atoms in each of the molecules in the files.\n",
    "3. Normalizes the counts to values between 0 and 1 using tf.Transform.\n",
    "4. Partitions the dataset into a training dataset and an evaluation dataset.\n",
    "5. Writes the two datasets as TFRecord objects.\n",
    "\n",
    "Apache Beam transforms can efficiently manipulate single elements at a time, but transforms that require a full pass of the dataset cannot easily be done with only Apache Beam and are better done using [tf.Transform](https://www.tensorflow.org/tfx/guide/tft). Because of this, the code uses Apache Beam transforms to read and format the molecules, and to count the atoms in each molecule. The code then uses `tf.Transform` to find the global minimum and maximum counts in order to normalize the data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
